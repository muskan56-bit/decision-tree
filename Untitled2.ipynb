{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Tree"
      ],
      "metadata": {
        "id": "MCLZ-PoueobL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "q.no.1) What is a Decision Tree, and how does it work in the context of\n",
        "classification?"
      ],
      "metadata": {
        "id": "E5ZT3gzbe6tr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> A Decision Tree  is a non-parametric supervised learning method used in machine learning for both classification and regression tasks. Its goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features.\n",
        "In the context of classification, a Decision Tree works by recursively splitting the dataset into more homogeneous subsets based on the features until a stopping criterion is met. This process is often called Decision Tree Induction -:\n",
        "1.Start with the Root Node\n",
        "2.Determine the Best Split\n",
        "3.Create Child Nodes and Recurse\n",
        "4.Stop Splitting (Stopping Criteria)\n",
        "5.Classification\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5X0mEZ46gFfy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "q.no.2: Explain the concepts of Gini Impurity and Entropy as impurity measures.\n",
        "How do they impact the splits in a Decision Tree?"
      ],
      "metadata": {
        "id": "Fi1HKZcwe_Mr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> Gini Impurity and Entropy are the primary metrics used by Decision Tree algorithms to determine the best way to split a node (data subset) into sub-nodes. They are measures of purity or homogeneity within a set of data points.\n",
        "gini impurity: The Gini Impurity measures how often a randomly chosen element from the set would be incorrectly labeled if it were randomly labeled according to the distribution of labels in the subset.\n",
        "\n",
        "Impact on Splits:When the Decision Tree algorithm evaluates a potential split, it calculates the Weighted Gini Impurity of the resulting child nodes. It selects the split that results in the lowest overall weighted Gini Impurity among the children.\n",
        "\n",
        "Entropy:Entropy is a concept borrowed from information theory, and it measures the randomness or unpredictability in a dataset.\n",
        "\n",
        "Impact on Splits: The Decision Tree doesn't directly look for the lowest entropy, but rather the split that provides the highest Information Gain (IG)."
      ],
      "metadata": {
        "id": "6JvFikjIhSP9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "q.no. 3: What is the difference between Pre-Pruning and Post-Pruning in Decision\n",
        "Trees? Give one practical advantage of using each."
      ],
      "metadata": {
        "id": "LhbL4-N-fDCa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> Pre-Pruning : Pre-Pruning involves halting the growth of the decision tree before it is fully induced (grown to its maximum possible depth). You set constraints before or during the tree-building process to stop splits that are not deemed beneficial.\n",
        "\n",
        "primary practical advantage of pre-pruning is Computational Efficiency and Speed\n",
        "\n",
        "Post-Pruning:Post-Pruning involves growing the decision tree fully (allowing it to overfit the training data) and then systematically trimming back the non-significant branches or subtrees by replacing them with leaf nodes.\n",
        "\n",
        "The primary practical advantage of post-pruning is Optimal Generalization and Higher Accuracy.\n"
      ],
      "metadata": {
        "id": "IYZL8TSVjEiA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "q.no.4: What is Information Gain in Decision Trees, and why is it important for\n",
        "choosing the best split?\n"
      ],
      "metadata": {
        "id": "_x9ZBevCfIji"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "->Information Gain is mathematically defined as the difference between the Entropy of the parent node (before the split) and the weighted average Entropy of the child nodes (after the split).\n",
        "\n",
        "importance for choosing : a Decision Tree algorithm is to grow the tree in a way that creates the purest possible leaf nodes with the minimum number of splits. Information Gain guides this greedy search process at every single node.\n",
        "1. Selection Criterion\n",
        "2. Maximization\n",
        "3. Impurity Reduction"
      ],
      "metadata": {
        "id": "fmWcnu57kSMc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "q.no.5: What are some common real-world applications of Decision Trees, and\n",
        "what are their main advantages and limitations?\n"
      ],
      "metadata": {
        "id": "cVOX8zOJfMNN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> common real- world application\n",
        "1. Finance / Banking - Credit Risk Assessment\n",
        "2.Healthcare / Medicine - Disease Diagnosis\n",
        "3. E-commerce / Marketing - Customer Churn Prediction\n",
        "4. Fraud Detection - Transaction Screening\n",
        "5. Manufacturing - Quality Control"
      ],
      "metadata": {
        "id": "KfZXB3FblaUo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "q.no.6: Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Train a Decision Tree Classifier using the Gini criterion\n",
        "● Print the model’s accuracy and feature importances"
      ],
      "metadata": {
        "id": "okTjnI1ffQlS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "q.no.7: Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n",
        "a fully-grown tree.\n"
      ],
      "metadata": {
        "id": "N0no2b7cfa1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "feature_names = iris.feature_names\n",
        "target_names = iris.target_names\n",
        "\n",
        "print(\"--- Iris Dataset Loaded ---\")\n",
        "print(f\"Features: {feature_names}\")\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "dt_classifier = DecisionTreeClassifier(\n",
        "    criterion='gini',\n",
        "    random_state=42,\n",
        "    max_depth=3\n",
        ")\n",
        "\n",
        "print(\"\\nTraining Decision Tree Classifier (Gini Criterion)...\")\n",
        "dt_classifier.fit(X_train, y_train)\n",
        "print(\"Training complete.\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "y_pred = dt_classifier.predict(X_test)\n",
        "\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"\\n--- Model Evaluation ---\")\n",
        "print(f\"Model Accuracy on Test Set: **{accuracy:.4f}**\")\n",
        "\n",
        "\n",
        "importances = dt_classifier.feature_importances_\n",
        "\n",
        "print(\"\\n--- Feature Importances ---\")\n",
        "\n",
        "feature_importances = sorted(\n",
        "    zip(feature_names, importances),\n",
        "    key=lambda x: x[1],\n",
        "    reverse=True\n",
        ")\n",
        "\n",
        "for name, score in feature_importances:\n",
        "    print(f\"{name:<20}: {score:.4f}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bh32rvGbpPLZ",
        "outputId": "726d0b34-6b03-4f54-dfa9-8bc2446759de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Iris Dataset Loaded ---\n",
            "Features: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
            "\n",
            "Training Decision Tree Classifier (Gini Criterion)...\n",
            "Training complete.\n",
            "\n",
            "--- Model Evaluation ---\n",
            "Model Accuracy on Test Set: **1.0000**\n",
            "\n",
            "--- Feature Importances ---\n",
            "petal length (cm)   : 0.9346\n",
            "petal width (cm)    : 0.0654\n",
            "sepal length (cm)   : 0.0000\n",
            "sepal width (cm)    : 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "q.no8: Write a Python program to:\n",
        "● Load the Boston Housing Dataset\n",
        "● Train a Decision Tree Regressor\n",
        "● Print the Mean Squared Error (MSE) and feature importances\n"
      ],
      "metadata": {
        "id": "z-CMb6csflAs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import warnings\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data\n",
        "y = housing.target\n",
        "feature_names = housing.feature_names\n",
        "\n",
        "print(\"--- California Housing Dataset Loaded (Replacement for Boston Housing) ---\")\n",
        "print(f\"Features: {feature_names}\")\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "dt_regressor = DecisionTreeRegressor(\n",
        "    random_state=42,\n",
        "    max_depth=8\n",
        ")\n",
        "\n",
        "print(\"\\nTraining Decision Tree Regressor...\")\n",
        "dt_regressor.fit(X_train, y_train)\n",
        "print(\"Training complete.\")\n",
        "\n",
        "\n",
        "y_pred = dt_regressor.predict(X_test)\n",
        "\n",
        "\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "\n",
        "print(\"\\n--- Model Evaluation Metrics ---\")\n",
        "print(f\"Mean Squared Error (MSE): **{mse:.4f}** (Lower is better)\")\n",
        "print(f\"R-squared Score (R²): **{r2:.4f}** (Closer to 1.0 is better)\")\n",
        "\n",
        "\n",
        "importances = dt_regressor.feature_importances_\n",
        "\n",
        "print(\"\\n--- Feature Importances (Regressor) ---\")\n",
        "\n",
        "feature_importances = sorted(\n",
        "    zip(feature_names, importances),\n",
        "    key=lambda x: x[1],\n",
        "    reverse=True\n",
        ")\n",
        "\n",
        "for name, score in feature_importances:\n",
        "    print(f\"{name:<20}: {score:.4f}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnjQGEL3p4KY",
        "outputId": "1017095b-22bf-40b8-fbd9-16284152fd05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- California Housing Dataset Loaded (Replacement for Boston Housing) ---\n",
            "Features: ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']\n",
            "\n",
            "Training Decision Tree Regressor...\n",
            "Training complete.\n",
            "\n",
            "--- Model Evaluation Metrics ---\n",
            "Mean Squared Error (MSE): **0.4220** (Lower is better)\n",
            "R-squared Score (R²): **0.6779** (Closer to 1.0 is better)\n",
            "\n",
            "--- Feature Importances (Regressor) ---\n",
            "MedInc              : 0.6629\n",
            "AveOccup            : 0.1321\n",
            "Latitude            : 0.0613\n",
            "Longitude           : 0.0502\n",
            "HouseAge            : 0.0422\n",
            "AveRooms            : 0.0341\n",
            "AveBedrms           : 0.0089\n",
            "Population          : 0.0082\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "q.no.9: Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Tune the Decision Tree’s max_depth and min_samples_split using\n",
        "GridSearchCV\n",
        "● Print the best parameters and the resulting model accuracy"
      ],
      "metadata": {
        "id": "qZgIJzD5fnhL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "feature_names = iris.feature_names\n",
        "\n",
        "print(\"--- Iris Dataset Loaded ---\")\n",
        "print(f\"Features: {feature_names}\")\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, 6, 7],\n",
        "    'min_samples_split': [2, 5, 10, 15, 20]\n",
        "}\n",
        "\n",
        "print(\"\\nStarting GridSearchCV to find optimal hyperparameters...\")\n",
        "\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=dt_classifier,\n",
        "    param_grid=param_grid,\n",
        "    scoring='accuracy',\n",
        "    cv=5,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "best_params = grid_search.best_params_\n",
        "print(\"\\n--- GridSearchCV Results ---\")\n",
        "print(f\"Best Parameters Found: **{best_params}**\")\n",
        "\n",
        "best_cv_score = grid_search.best_score_\n",
        "print(f\"Best Cross-Validation Accuracy (on training data): **{best_cv_score:.4f}**\")\n",
        "\n",
        "\n",
        "best_dt_model = grid_search.best_estimator_\n",
        "y_pred_test = best_dt_model.predict(X_test)\n",
        "final_test_accuracy = accuracy_score(y_test, y_pred_test)\n",
        "\n",
        "print(f\"Final Model Accuracy on Test Set: **{final_test_accuracy:.4f}**\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRgPR9DMtqdh",
        "outputId": "ae41d5a3-2f37-48cf-d67e-bfd35ed2407c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Iris Dataset Loaded ---\n",
            "Features: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
            "\n",
            "Starting GridSearchCV to find optimal hyperparameters...\n",
            "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
            "\n",
            "--- GridSearchCV Results ---\n",
            "Best Parameters Found: **{'max_depth': 4, 'min_samples_split': 2}**\n",
            "Best Cross-Validation Accuracy (on training data): **0.9417**\n",
            "Final Model Accuracy on Test Set: **1.0000**\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "q.no. 10: Imagine you’re working as a data scientist for a healthcare company that\n",
        "wants to predict whether a patient has a certain disease. You have a large dataset with\n",
        "mixed data types and some missing values.\n",
        "Explain the step-by-step process you would follow to:\n",
        "● Handle the missing values\n",
        "● Encode the categorical features\n",
        "● Train a Decision Tree model\n",
        "● Tune its hyperparameters\n",
        "● Evaluate its performance\n",
        "And describe what business value this model could provide in the real-world\n",
        "setting.\n"
      ],
      "metadata": {
        "id": "5GUD30IUfraE"
      }
    }
  ]
}